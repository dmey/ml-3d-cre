{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "# https://stackoverflow.com/a/52897289\n",
    "\n",
    "import os\n",
    "\n",
    "iteration_idx = int(os.environ.get('ITERATION', 0))\n",
    "\n",
    "# Seed value\n",
    "seed_value = iteration_idx\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "import hashlib\n",
    "import math\n",
    "from typing import NamedTuple\n",
    "from pathlib import Path, PosixPath\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from IPython.display import Image\n",
    "\n",
    "%aimport utils\n",
    "\n",
    "# Note: Start jupyter with CUDA_VISIBLE_DEVICES=-1 env var to force CPU mode.\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    print('Not using GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthia as syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_datasets = False\n",
    "enhance_data = True\n",
    "cloudy_profiles_only = True\n",
    "is_windows = os.name == 'nt'\n",
    "\n",
    "pbs_id = str(os.environ.get('PBS_JOBID', '1'))\n",
    "pbs_array_idx = str(os.environ.get('PBS_ARRAY_INDEX', '0'))\n",
    "is_pbs_job = 'PBS_JOBID' in os.environ \n",
    "\n",
    "if is_windows:\n",
    "    wsl = 'wsl'\n",
    "else:\n",
    "    wsl = ''\n",
    "\n",
    "path_proj = Path.cwd().parent\n",
    "path_data = path_proj / 'data'\n",
    "\n",
    "path_logs = Path(tempfile.gettempdir()) if is_pbs_job else path_proj / 'logs'\n",
    "path_logs.mkdir(exist_ok=True)\n",
    "\n",
    "path_results = path_proj / 'results'\n",
    "path_results.mkdir(exist_ok=True)\n",
    "\n",
    "path_common = path_data / 'common'\n",
    "path_saf = path_data / 'saf'\n",
    "path_saf_out_dir = Path(tempfile.gettempdir()) if is_pbs_job else path_saf\n",
    "\n",
    "path_saf_in =  path_saf / 'nwp_saf_profiles_in.nc'\n",
    "path_saf_in_synth = path_saf_out_dir / 'nwp_saf_profiles_in_synth.nc'\n",
    "\n",
    "path_ecrad = path_proj / 'ecrad'\n",
    "path_ecrad_bin = path_ecrad / 'bin' / 'ecrad'\n",
    "\n",
    "path_triple_nml = path_saf / 'configCY47R1_tripleclouds.nam'\n",
    "path_sparta_nml = path_saf / 'configCY47R1_spartacus.nam'\n",
    "path_triple_out_synth = path_saf_out_dir / 'ecrad_nwp_saf_profiles_tripleclouds_out_synth.nc'\n",
    "path_sparta_out_synth = path_saf_out_dir / 'ecrad_nwp_saf_profiles_spartacus_out_synth.nc'\n",
    "window_paths = [path_ecrad, path_ecrad_bin, path_triple_nml, path_sparta_nml, path_saf_in_synth, path_triple_out_synth, path_sparta_out_synth, path_logs]\n",
    "posix_paths = []\n",
    "for path in window_paths:\n",
    "    if is_windows:\n",
    "        x = ! wsl wslpath \"{path}\"\n",
    "        x = x[0]\n",
    "    else:\n",
    "        x = path.as_posix()\n",
    "    posix_paths.append(x)\n",
    "\n",
    "# Note: See first cell for ITERATION env var.\n",
    "    \n",
    "use_diff = int(os.environ.get('USE_DIFF', 1))\n",
    "\n",
    "if use_diff:\n",
    "    use_pressure_level = int(os.environ.get('USE_PRESSURE_LEVEL', 5000)) # Pa\n",
    "else:\n",
    "    use_pressure_level = 0\n",
    "\n",
    "ncores = int(os.environ.get('NCORES', cpu_count()))\n",
    "\n",
    "# Copula vars\n",
    "copula_type = os.environ.get('COPULA_TYPE', 'gaussian')\n",
    "var_synth = os.environ.get('VAR_SYNTH', 'sw_albedo,cos_solar_zenith_angle').split(',')\n",
    "unif_ratio = float(os.environ.get('UNIF_RATIO', 1.0))\n",
    "stretch_factor = float(os.environ.get('STRETCH_FACTOR', 1.0))\n",
    "synth_mul_factor = int(os.environ.get('SYNTH_MUL_FACTOR', 0))\n",
    "\n",
    "# ML vars\n",
    "save_model = os.environ.get('SAVE_MODEL', '1') == '1'\n",
    "var_ml = os.environ.get('VAR_ML', 'optical_depth_fl,cos_solar_zenith_angle,sw_albedo,skin_temperature,cloud_fraction,temperature_fl,q,dz').split(',')\n",
    "epochs = int(os.environ.get('EPOCHS', 2))\n",
    "model_type = os.environ.get('MODEL_TYPE', 'MLP') # 'RNN', 'MLP'\n",
    "rnn_type = os.environ.get('RNN_TYPE', 'GRU') # 'GRU', SimpleRNN', 'LSTM'\n",
    "rnn_direction = os.environ.get('RNN_DIRECTION', 'bi') # 'fwd', 'bwd', 'bi'\n",
    "n_hidden_layers = int(os.environ.get('N_HIDDEN_LAYERS', 3))\n",
    "hidden_size = float(os.environ.get('HIDDEN_SIZE', 1))\n",
    "activation = str(os.environ.get('ACTIVATION', 'elu'))\n",
    "loss = str(os.environ.get('LOSS', 'mse'))\n",
    "l1_penalty = float(os.environ.get('L1_PENALTY', 1e-5)) \n",
    "l2_penalty = float(os.environ.get('L2_PENALTY', 1e-5))\n",
    "var_regularizer_factor = float(os.environ.get('VAR_REGULARIZER_FACTOR', 0))\n",
    "dropout_ratio_input = float(os.environ.get('DROPOUT_RATIO_INPUT', 0))\n",
    "dropout_ratio_hidden = float(os.environ.get('DROPOUT_RATIO_HIDDEN', 0))\n",
    "use_heating_rates = int(os.environ.get('USE_HEATING_RATES', '1'))\n",
    "\n",
    "job_name = str(os.environ.get('JOB_NAME', 'default'))\n",
    "\n",
    "input_path = path_saf_in_synth\n",
    "triple_output_path = path_triple_out_synth\n",
    "sparta_output_path = path_sparta_out_synth\n",
    "shuffle = False\n",
    "    \n",
    "case_name = str(os.environ.get('CASE_NAME', 'split'))\n",
    "\n",
    "if case_name == 'combined':\n",
    "    is_lw_sw_split = 0\n",
    "elif case_name == 'split':\n",
    "    is_lw_sw_split = 1\n",
    "else:\n",
    "    raise RuntimeError('Invalid case name')\n",
    "\n",
    "case_name_without_iteration = (\n",
    "    f'case={case_name},' +\n",
    "    f'use_diff={use_diff},' +\n",
    "    f'use_heating_rates={use_heating_rates},' +\n",
    "    f'copula_type={copula_type},' +\n",
    "    f'synth_mul_factor={synth_mul_factor},' +\n",
    "    f'unif_ratio={unif_ratio},' +\n",
    "    f'stretch_factor={stretch_factor},' + \n",
    "    f'var_synth={\",\".join(var_synth)},' +\n",
    "    f'var_ml={\",\".join(var_ml)},' + \n",
    "    f'loss={loss},' +\n",
    "    f'activation={activation},' +\n",
    "    f'l1_penalty={l1_penalty},' +\n",
    "    f'l2_penalty={l2_penalty},' +\n",
    "    f'var_regularizer_factor={var_regularizer_factor},' +\n",
    "    f'dropout_ratio_input={dropout_ratio_input},' +\n",
    "    f'dropout_ratio_hidden={dropout_ratio_hidden},' +\n",
    "    f'model_type={model_type},' +\n",
    "    f'rnn_type={rnn_type},' +\n",
    "    f'rnn_direction={rnn_direction},' +\n",
    "    f'layers={n_hidden_layers},' +\n",
    "    f'size={hidden_size}'\n",
    ")\n",
    "case_name = case_name_without_iteration + f',iteration={iteration_idx}'\n",
    "case_name_hashed = hashlib.md5(case_name.encode('ascii')).hexdigest()\n",
    "   \n",
    "stat_quantities_sw = ['flux_dn_direct_sw', 'flux_dn_sw', 'flux_up_sw', 'heating_rate_sw']\n",
    "_heating_rate_sw = ['heating_rate_sw']\n",
    "stat_quantities_lw = ['flux_dn_lw', 'flux_up_lw', 'heating_rate_lw']\n",
    "_heating_rate_lw = ['heating_rate_lw']\n",
    "\n",
    "(case_name, case_name_hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_pkl_path = path_results / f\"job_stats_{job_name}\" / (case_name_hashed + '.pkl')\n",
    "\n",
    "if is_pbs_job and stats_pkl_path.exists():\n",
    "    print('job results exist already, exiting')\n",
    "    raise RuntimeError('early stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty file to keep track of job failures\n",
    "if is_pbs_job:\n",
    "    f_job_failure = path_results / f\"job_failures_{job_name}\" / f'{case_name_hashed}_{pbs_id}_{pbs_array_idx}'\n",
    "    if not f_job_failure.parent.exists():\n",
    "        f_job_failure.parent.mkdir()\n",
    "    with open(f_job_failure, 'w') as f:\n",
    "        f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Enhancment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_cloudy_profiles(ds, cloudy=True):\n",
    "    is_cloudy = ds['cloud_fraction'].max('level') > 0\n",
    "    if cloudy:\n",
    "        return ds.sel(column=is_cloudy)\n",
    "    else:\n",
    "        return ds.sel(column=~is_cloudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extract_datasets:\n",
    "    ! {wsl} find -name \"*.nc.xz\" -exec unxz -k '\\{\\}' \\\n",
    "\n",
    "if enhance_data:\n",
    "    # Load profiles\n",
    "    ds_true = xr.open_dataset(path_saf_in)\n",
    "    \n",
    "    if cloudy_profiles_only:\n",
    "        ds_true = sel_cloudy_profiles(ds_true)\n",
    "    \n",
    "    display(ds_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    ds_train, ds_temp = utils.train_test_split_dataset(ds_true, train_size=0.6, dim='column', shuffle=True, seed=42)\n",
    "    ds_test, ds_validation = utils.train_test_split_dataset(ds_temp, test_size=0.5, dim='column', shuffle=True, seed=42)\n",
    "    print(len(ds_train.column), len(ds_validation.column), len(ds_test.column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Generate a new dataset with x the number of profiles. NB: only albedo\n",
    "    # and solar angle are synthetic, other vars are just duplicated.\n",
    "\n",
    "    ds_train_synth = utils.compute_augmented_dataset(\n",
    "        ds_train, var_synth,\n",
    "        synth_mul_factor=synth_mul_factor,\n",
    "        uniformization_ratio=unif_ratio,\n",
    "        stretch_factor=stretch_factor,\n",
    "        copula_type=copula_type,\n",
    "        num_threads=ncores\n",
    "    )\n",
    "\n",
    "    ds_train_synth['sw_albedo'].plot.hist(); plt.show();\n",
    "    ds_train_synth['cos_solar_zenith_angle'].plot.hist(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Add Test dataset to merged generated random saf_profiles\n",
    "    ds_synth = xr.concat([ds_train, ds_train_synth, ds_validation, ds_test], dim='column')\n",
    "\n",
    "    # When concatenating xarray adds 'column' dim to o2_vmr (1D) so we replace with that from original dataset\n",
    "    ds_synth['o2_vmr'] = ds_true['o2_vmr']\n",
    "\n",
    "    print(len(ds_synth.column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    ds_synth.to_netcdf(path_saf_in_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Spartacus\n",
    "    # ! cd {ecrad_dir} && wsl OMP_NUM_THREADS= {ecrad} {sparta_nml_path} {derived_input_path} {sparta_output_path} > sparta_synth.log\n",
    "    ! cd {path_ecrad} && {wsl} OMP_NUM_THREADS= {posix_paths[1]} {posix_paths[3]} {posix_paths[4]} {posix_paths[6]} > {window_paths[7]}/sparta_synth.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Tripleclouds\n",
    "    ! cd {path_ecrad} && {wsl} OMP_NUM_THREADS= {posix_paths[1]} {posix_paths[2]} {posix_paths[4]} {posix_paths[5]} > {window_paths[7]}/triple_synth.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    new_train_size_with_synthetic = len(ds_train.column) + len(ds_train_synth.column)\n",
    "else:\n",
    "    new_train_size_with_synthetic = 13703 #This number is generated with the forumla above for creating 5 times the number of samples.\n",
    "\n",
    "print(new_train_size_with_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_synth = utils.load_scheme_inputs(path_saf_in_synth, only_relevant=False)\n",
    "\n",
    "if use_pressure_level:\n",
    "    level = int(np.fabs(ds_synth.pressure_fl.mean(axis=0) -use_pressure_level).argmin())\n",
    "    level = range(0, int(level) + 1) # End of range is exclusive\n",
    "else:\n",
    "    level = None\n",
    "display(level)\n",
    "\n",
    "ds_synth = utils.add_derived_inputs(ds_synth)\n",
    "\n",
    "input_quantities = var_ml\n",
    "\n",
    "# Select input quantities\n",
    "ds_synth = ds_synth[input_quantities]\n",
    "\n",
    "# Subset input levels\n",
    "if level:\n",
    "    ds_synth = ds_synth.sel(level=level)\n",
    "\n",
    "# Normalize inputs\n",
    "normalize_inputs = True\n",
    "norm_root = 1\n",
    "if normalize_inputs:\n",
    "    norm_stats_in = utils.compute_norm_stats(ds_synth, norm_root)\n",
    "    ds_synth = utils.normalize_inputs(ds_synth, norm_stats_in)\n",
    "\n",
    "utils.plot_inputs(ds_synth, normalize_inputs)\n",
    "\n",
    "\n",
    "ds_train_synth, ds_temp = utils.train_test_split_dataset(ds_synth, train_size=new_train_size_with_synthetic, dim='column', shuffle=False)\n",
    "ds_validation, ds_test = utils.train_test_split_dataset(ds_temp, test_size=0.5, dim='column', shuffle=False)\n",
    "\n",
    "print(len(ds_train_synth.column), len(ds_validation.column), len(ds_test.column))\n",
    "\n",
    "column_training_range = slice(0, len(ds_train_synth.column))\n",
    "column_training_validation_range = slice(0, len(ds_train_synth.column) + len(ds_validation.column))\n",
    "column_test_range = slice(len(ds_train_synth.column) + len(ds_validation.column), None)\n",
    "print(column_training_validation_range)\n",
    "print(column_test_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical scheme inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_1d_to_2d_vars(ds, var_names):\n",
    "    n_columns = ds.dims['column']\n",
    "    n_levels = ds.dims['level']\n",
    "    for var_name in var_names:\n",
    "        da = ds[var_name]\n",
    "        arr = np.empty((n_columns, n_levels), np.float64)\n",
    "        arr[:,:] = da.values.reshape(n_columns, 1)\n",
    "        da = xr.DataArray(arr, dims=['column', 'level'])\n",
    "        ds = ds.assign({var_name: da})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available inputs. We load them so that we can save them with the data later\n",
    "ds_inputs_all = utils.load_scheme_inputs(path_saf_in_synth, only_relevant=False)\n",
    "\n",
    "# For training, we use a subset of inputs only.\n",
    "if is_lw_sw_split:\n",
    "    ds_inputs_lw = ds_synth.drop(['cos_solar_zenith_angle', 'sw_albedo'])\n",
    "    ds_inputs_sw = ds_synth.drop(['skin_temperature', 'temperature_fl'])\n",
    "    if model_type == 'RNN':\n",
    "        ds_inputs_lw = xr_1d_to_2d_vars(ds_inputs_lw, ['skin_temperature'])\n",
    "        ds_inputs_sw = xr_1d_to_2d_vars(ds_inputs_sw, ['cos_solar_zenith_angle', 'sw_albedo'])\n",
    "    display(ds_inputs_lw, ds_inputs_sw)\n",
    "else:\n",
    "    ds_inputs = ds_synth\n",
    "    display(ds_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical scheme outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_outputs = True\n",
    "\n",
    "if normalize_outputs:\n",
    "    if use_diff:\n",
    "        # Do not apply root 8 since values can be negative.\n",
    "        norm_root_out = 1\n",
    "    else:\n",
    "        norm_root_out = norm_root\n",
    "\n",
    "if is_lw_sw_split:\n",
    "    # Load LW outputs\n",
    "    lw_fluxes = ['flux_up_lw', 'flux_dn_lw']\n",
    "    ds_triple_lw = utils.load_scheme_outputs(triple_output_path)[lw_fluxes]\n",
    "    ds_sparta_lw = utils.load_scheme_outputs(sparta_output_path)[lw_fluxes]\n",
    "    \n",
    "    # Difference between fast and slow physical scheme outputs\n",
    "    if use_diff:\n",
    "        ds_outputs_lw = ds_sparta_lw - ds_triple_lw\n",
    "    else:\n",
    "        ds_outputs_lw = ds_sparta_lw\n",
    "    \n",
    "    if use_heating_rates:\n",
    "        ds_outputs_lw = utils.add_heating_rates(ds_outputs_lw, ds_inputs_all)\n",
    "    \n",
    "    # Subset levels\n",
    "    if level:\n",
    "        ds_outputs_lw = ds_outputs_lw.sel(half_level=level)\n",
    "        if use_heating_rates:\n",
    "            ds_outputs_lw = ds_outputs_lw.sel(level=level)\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize_outputs:   \n",
    "        norm_stats_out_lw = utils.compute_norm_stats(ds_outputs_lw, norm_root_out)\n",
    "        ds_outputs_lw = utils.normalize_outputs(ds_outputs_lw, norm_stats_out_lw)\n",
    "    \n",
    "    # Load SW outputs\n",
    "    sw_fluxes = ['flux_up_sw', 'flux_dn_sw', 'flux_dn_direct_sw']\n",
    "    ds_triple_sw = utils.load_scheme_outputs(triple_output_path)[sw_fluxes]\n",
    "    ds_sparta_sw = utils.load_scheme_outputs(sparta_output_path)[sw_fluxes]\n",
    "\n",
    "    # Difference between fast and slow physical scheme outputs\n",
    "    if use_diff:\n",
    "        ds_outputs_sw = ds_sparta_sw - ds_triple_sw\n",
    "    else:\n",
    "        ds_outputs_sw = ds_sparta_sw\n",
    "    \n",
    "    if use_heating_rates:\n",
    "        ds_outputs_sw = utils.add_heating_rates(ds_outputs_sw, ds_inputs_all)\n",
    "    \n",
    "    # Subset levels\n",
    "    if level:\n",
    "        ds_outputs_sw = ds_outputs_sw.sel(half_level=level)\n",
    "        if use_heating_rates:\n",
    "            ds_outputs_sw = ds_outputs_sw.sel(level=level)\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize_outputs:\n",
    "        norm_stats_out_sw = utils.compute_norm_stats(ds_outputs_sw, norm_root_out)\n",
    "        ds_outputs_sw = utils.normalize_outputs(ds_outputs_sw, norm_stats_out_sw)\n",
    "    \n",
    "    display(ds_outputs_lw, ds_outputs_sw)\n",
    "\n",
    "else:\n",
    "    # Load outputs\n",
    "    ds_triple = utils.load_scheme_outputs(triple_output_path)\n",
    "    ds_sparta = utils.load_scheme_outputs(sparta_output_path)\n",
    "\n",
    "    # Difference between fast and slow physical scheme outputs\n",
    "    if use_diff:\n",
    "        ds_outputs = ds_sparta - ds_triple\n",
    "    else:\n",
    "        ds_outputs = ds_sparta\n",
    "    \n",
    "    # Normalize\n",
    "    if normalize_outputs:\n",
    "        norm_stats_out = utils.compute_norm_stats(ds_outputs, norm_root_out)\n",
    "        ds_outputs = utils.normalize_outputs(ds_outputs, norm_stats_out)\n",
    "    \n",
    "    display(ds_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(ds_x_inputs, ds_y_inputs, shuffle, test_size, apply_pca=False):\n",
    "    \n",
    "    train_data = utils.create_training_data(\n",
    "        ds_x_inputs, ds_y_inputs,\n",
    "        apply_pca=apply_pca,\n",
    "        shuffle=False,\n",
    "        test_size=test_size,\n",
    "        x_flat_dims=2 if model_type == 'RNN' else 1\n",
    "    )\n",
    "    print(train_data.x_train_flat.shape)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def sel_train_val(ds):\n",
    "    return ds.sel(column=column_training_validation_range)\n",
    "\n",
    "def sel_train_test(ds):\n",
    "    # ds_synth['o2_vmr'] = ds_true['o2_vmr']\n",
    "    return xr.concat([ds.sel(column=column_training_range), ds.sel(column=column_test_range)], dim='column')\n",
    "\n",
    "if is_lw_sw_split:\n",
    "    train_val_data_lw = create_train_data(sel_train_val(ds_inputs_lw), sel_train_val(ds_outputs_lw), shuffle, test_size=len(ds_validation.column))\n",
    "    train_val_data_sw = create_train_data(sel_train_val(ds_inputs_sw), sel_train_val(ds_outputs_sw), shuffle, test_size=len(ds_validation.column))\n",
    "    train_test_data_lw = create_train_data(sel_train_test(ds_inputs_lw), sel_train_test(ds_outputs_lw), shuffle, test_size=len(ds_test.column))\n",
    "    train_test_data_sw = create_train_data(sel_train_test(ds_inputs_sw), sel_train_test(ds_outputs_sw), shuffle, test_size=len(ds_test.column))\n",
    "else:   \n",
    "    train_val_data = create_train_data(sel_train_val(ds_inputs), sel_train_val(ds_outputs), shuffle, test_size=len(ds_validation.column))\n",
    "    train_test_data = create_train_data(sel_train_test(ds_inputs), sel_train_test(ds_outputs), shuffle, test_size=len(ds_validation.column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_lw = int(hidden_size * train_val_data_lw.x_train_flat.shape[1])\n",
    "hidden_size_sw = int(hidden_size * train_val_data_sw.x_train_flat.shape[1])\n",
    "print(hidden_size_lw, hidden_size_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def train_and_save_model(case_name,\n",
    "                         train_val_data, train_test_data,\n",
    "                         ds_inputs_all_train_test, ds_triple_train_test, ds_sparta_train_test,\n",
    "                         model_type, rnn_type, rnn_direction,\n",
    "                         n_hidden_layers, hidden_size, activation, l1_penalty,\n",
    "                         l2_penalty, var_regularizer_factor, dropout_ratio_input=0.0, dropout_ratio_hidden=0.0, \n",
    "                         learning_rate=0.001, epochs=epochs, save_model=False):\n",
    "\n",
    "    hp = dict(model_type=model_type,\n",
    "              rnn_type=rnn_type,\n",
    "              rnn_direction=rnn_direction,\n",
    "              n_hidden_layers=n_hidden_layers,\n",
    "              hidden_size=hidden_size,\n",
    "              activation=activation,\n",
    "              dropout_ratio_input=dropout_ratio_input,\n",
    "              dropout_ratio_hidden=dropout_ratio_hidden,\n",
    "              l1_penalty=l1_penalty,\n",
    "              l2_penalty=l2_penalty,\n",
    "              var_regularizer_factor=var_regularizer_factor,\n",
    "              learning_rate=learning_rate,\n",
    "              loss=loss)\n",
    "\n",
    "    if model_type == 'RNN':\n",
    "        model_fn = utils.build_rnn_model\n",
    "    elif model_type == 'MLP':\n",
    "        model_fn = utils.build_dense_nn_model\n",
    "    else:\n",
    "        raise ValueError('unknown model type: ' + model_type)\n",
    "\n",
    "    trained_model = utils.train_model(model_fn,\n",
    "                                      train_val_data,\n",
    "                                      hp,\n",
    "                                      epochs=epochs,\n",
    "                                      batch_size=256,\n",
    "                                      early_stopping=True,\n",
    "                                      verbose=2,\n",
    "                                      save_best=True)\n",
    "\n",
    "    d = utils.save_training_results(case_name,\n",
    "                                    trained_model,\n",
    "                                    train_test_data,\n",
    "                                    ds_inputs_all_train_test,\n",
    "                                    ds_triple_train_test,\n",
    "                                    ds_sparta_train_test,\n",
    "                                    skip_save=not save_model,\n",
    "                                    save_dir=str(path_results))\n",
    "    return d, trained_model\n",
    "\n",
    "if is_lw_sw_split:\n",
    "    d_lw, trained_model_lw = train_and_save_model(\n",
    "        'model_lw',\n",
    "        train_val_data_lw, train_test_data_lw,\n",
    "        sel_train_test(ds_inputs_all), sel_train_test(ds_triple_lw), sel_train_test(ds_sparta_lw),\n",
    "        model_type, rnn_type, rnn_direction,\n",
    "        n_hidden_layers, hidden_size_lw, activation, l1_penalty, l2_penalty, var_regularizer_factor, dropout_ratio_input, dropout_ratio_hidden, epochs=epochs, save_model=save_model)\n",
    "    \n",
    "    d_sw, trained_model_sw = train_and_save_model(\n",
    "        'model_sw',\n",
    "        train_val_data_sw, train_test_data_sw,\n",
    "        sel_train_test(ds_inputs_all), sel_train_test(ds_triple_sw), sel_train_test(ds_sparta_sw),\n",
    "        model_type, rnn_type, rnn_direction,\n",
    "        n_hidden_layers, hidden_size_sw, activation, l1_penalty, l2_penalty, var_regularizer_factor, dropout_ratio_input, dropout_ratio_hidden, epochs=epochs, save_model=save_model)    \n",
    "else:\n",
    "    d, trained_model = train_and_save_model(\n",
    "        'model',\n",
    "        train_val_data, train_test_data,\n",
    "        sel_train_test(ds_inputs_all), sel_train_test(ds_triple), sel_train_test(ds_sparta),\n",
    "        model_type, rnn_type, rnn_direction,\n",
    "        n_hidden_layers, hidden_size_sw, activation, l1_penalty, l2_penalty, var_regularizer_factor, dropout_ratio_input, dropout_ratio_hidden, save_model=save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_lw_sw_split:\n",
    "    trained_model_lw.history.plot()\n",
    "    trained_model_sw.history.plot()\n",
    "else:\n",
    "    trained_model.history.plot()\n",
    "    trained_model_history = trained_model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalize outputs\n",
    "vars_to_unnormalize = [\n",
    "     'y_true_train', 'y_true_test',\n",
    "     'y_pred_train', 'y_pred_test'\n",
    "]\n",
    "\n",
    "if normalize_outputs:\n",
    "    for name in vars_to_unnormalize:\n",
    "        if is_lw_sw_split:\n",
    "            d_lw[name] = utils.unnormalize_outputs(d_lw[name], norm_stats_out_lw)\n",
    "            d_sw[name] = utils.unnormalize_outputs(d_sw[name], norm_stats_out_sw)\n",
    "        else:\n",
    "            d[name] = utils.unnormalize_outputs(d[name], norm_stats_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "If the NN have been trained using a slice of pressure levels we recover the full profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results from the two NN\n",
    "def merge_lw_sw_split(ds_1, ds_2):\n",
    "    # At the moment this is a bit hacky as we create a new ds based on ds_1\n",
    "    # This means that the the combilne dataset will onyl have the NN specs from ds_1\n",
    "    # We do not need to merge 'x_train' and 'x_test' as these are all the inputs and therefore same for lw and sw \n",
    "    vars_to_merge = ['y_triple_train', 'y_triple_test',\n",
    "     'y_sparta_train', 'y_sparta_test', \n",
    "     'y_true_train', 'y_true_test',\n",
    "     'y_pred_train', 'y_pred_test']\n",
    " \n",
    "    d = ds_1 # This is a hack\n",
    "    for var in vars_to_merge:\n",
    "        if d.has_test_data:\n",
    "            d[var] = xr.merge([ds_1[var], ds_2[var]])\n",
    "        else:\n",
    "            if '_test' in var:\n",
    "                continue\n",
    "            else:\n",
    "                d[var] = xr.merge([ds_1[var], ds_2[var]])\n",
    "    return d\n",
    "\n",
    "if is_lw_sw_split:\n",
    "    d = merge_lw_sw_split(d_lw, d_sw)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.y_sparta_train = utils.add_heating_rates(d.y_sparta_train, d.x_train)\n",
    "d.y_sparta_test = utils.add_heating_rates(d.y_sparta_test, d.x_test)\n",
    "d.y_triple_train = utils.add_heating_rates(d.y_triple_train, d.x_train)\n",
    "d.y_triple_test = utils.add_heating_rates(d.y_triple_test, d.x_test)\n",
    "d.y_sparta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_levels(d_subset, d_template):\n",
    "    # Fill the missing values with zeros for downwelling fluxes -- i.e. the diff should be zero\n",
    "    # And propagate the value from the last calculated level from the inference to TOA for upwelling fluxes.\n",
    "    last_valid_level = len(d_subset.half_level) - 1\n",
    "    diff_full = xr.zeros_like(d_template)\n",
    "    for var in d_subset.keys():\n",
    "        # We only update values that have been calculated from the inference  -- all the others remain zero\n",
    "        diff_full[var][:, :last_valid_level + 1] = d_subset[var]\n",
    "        # In the case of upwelling fluxes, we need to propagate the last value calculated from the inference\n",
    "        # to the TOA to keep the physical meaning.\n",
    "        if '_up_' in var or 'heating_rate' in var:\n",
    "            diff_full[var][:, last_valid_level + 1:] = diff_full[var][:, last_valid_level:last_valid_level + 1]\n",
    "    diff_full = diff_full[[z for z in d_subset.variables]]\n",
    "    return diff_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pressure_level:\n",
    "    diff_pred_train = recover_levels(d.y_pred_train, d.y_sparta_train)\n",
    "    diff_pred_test = recover_levels(d.y_pred_test, d.y_sparta_test)\n",
    "    # Update the main data dictiorty with data for all the levels\n",
    "    d.y_pred_train = diff_pred_train\n",
    "    d.y_pred_test = diff_pred_test\n",
    "    # In the case of reference data -- i.e. for the difference -- we can calculate them directly as y_sparta_train were \n",
    "    # were saved with all the levels\n",
    "    d.y_true_train = d.y_sparta_train - d.y_triple_train\n",
    "    d.y_true_test = d.y_sparta_test - d.y_triple_test\n",
    "else:\n",
    "    pass\n",
    "d.y_true_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_heating_rates:\n",
    "    d.y_true_train = utils.add_heating_rates(d.y_true_train, d.x_train)\n",
    "    d.y_true_test = utils.add_heating_rates(d.y_true_test, d.x_test)\n",
    "    d.y_pred_train = utils.add_heating_rates(d.y_pred_train, d.x_train)\n",
    "    d.y_pred_test = utils.add_heating_rates(d.y_pred_test, d.x_test)\n",
    "d.y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if level:\n",
    "    m_lw = utils.compute_error_metrics(d.y_true_train.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_lw].drop(_heating_rate_lw),\n",
    "                                       d.y_pred_train.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_lw].drop(_heating_rate_lw)).add_suffix('_train')\n",
    "    m_lw = pd.concat([m_lw, utils.compute_error_metrics(d.y_true_test.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_lw].drop(_heating_rate_lw),\n",
    "                                                        d.y_pred_test.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_lw].drop(_heating_rate_lw)).add_suffix('_test')], 'columns')\n",
    "else:\n",
    "    m_lw = utils.compute_error_metrics(d.y_true_train[stat_quantities_lw].drop(_heating_rate_lw),\n",
    "                                       d.y_pred_train[stat_quantities_lw].drop(_heating_rate_lw)).add_suffix('_train')\n",
    "    m_lw = pd.concat([m_lw, utils.compute_error_metrics(d.y_true_test[stat_quantities_lw].drop(_heating_rate_lw),\n",
    "                                                        d.y_pred_test[stat_quantities_lw].drop(_heating_rate_lw)).add_suffix('_test')], 'columns')\n",
    "m_lw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if level:\n",
    "    m_sw = utils.compute_error_metrics(d.y_true_train.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_sw].drop(_heating_rate_sw),\n",
    "                                       d.y_pred_train.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_sw].drop(_heating_rate_sw)).add_suffix('_train')\n",
    "    m_sw = pd.concat([m_sw, utils.compute_error_metrics(d.y_true_test.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_sw].drop(_heating_rate_sw),\n",
    "                                                        d.y_pred_test.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1)).rename(level='half_level')[stat_quantities_sw].drop(_heating_rate_sw)).add_suffix('_test')], 'columns')\n",
    "else:\n",
    "    m_sw = utils.compute_error_metrics(d.y_true_train[stat_quantities_sw].drop(_heating_rate_sw),\n",
    "                                       d.y_pred_train[stat_quantities_sw].drop(_heating_rate_sw)).add_suffix('_train')\n",
    "    m_sw = pd.concat([m_sw, utils.compute_error_metrics(d.y_true_test[stat_quantities_sw].drop(_heating_rate_sw),\n",
    "                                                        d.y_pred_test[stat_quantities_sw].drop(_heating_rate_sw)).add_suffix('_test')], 'columns')\n",
    "m_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.y_true_train[stat_quantities_sw + stat_quantities_lw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.y_sparta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats to file to allow comparison between different runs.\n",
    "# See grid-stats.ipynb.\n",
    "\n",
    "case_stats = pd.concat([\n",
    "  pd.DataFrame({\n",
    "      'name': [case_name_without_iteration], # Useful for easy aggregation across iterations.\n",
    "      'pbs_id': [pbs_id],\n",
    "      'pbs_array_idx': [pbs_array_idx],\n",
    "      'job_name': [job_name],\n",
    "      'iteration': [iteration_idx],\n",
    "      'use_diff': [use_diff],\n",
    "      'use_heating_rates': [use_heating_rates],\n",
    "      'model_type': [model_type],\n",
    "      'rnn_type': [rnn_type],\n",
    "      'rnn_direction': [rnn_direction],\n",
    "      'hidden_size': [hidden_size],\n",
    "      'n_hidden_layers': [n_hidden_layers],\n",
    "      'copula_type': [copula_type],\n",
    "      'synth_mul_factor': [synth_mul_factor],\n",
    "      'unif_ratio': [unif_ratio],\n",
    "      'stretch_factor': [stretch_factor],\n",
    "      'loss': [loss],\n",
    "      'activation': [activation],\n",
    "      'l1_penalty': [l1_penalty],\n",
    "      'l2_penalty': [l2_penalty],\n",
    "      'var_regularizer_factor': [var_regularizer_factor],\n",
    "      'dropout_ratio_input': [dropout_ratio_input],\n",
    "      'dropout_ratio_hidden': [dropout_ratio_hidden],\n",
    "      'var_synth': [','.join(var_synth)],\n",
    "      'var_ml': [','.join(var_ml)],\n",
    "  }),\n",
    "  ((m_sw.loc[['all']] + m_lw.loc[['all']]) / 2).reset_index().drop(columns='quantity'),\n",
    "  m_sw.loc[['all']].reset_index().drop(columns='quantity').add_prefix('sw_'),\n",
    "  m_lw.loc[['all']].reset_index().drop(columns='quantity').add_prefix('lw_')\n",
    "], axis=1)\n",
    "display(case_stats)\n",
    "\n",
    "if not stats_pkl_path.parent.exists():\n",
    "    stats_pkl_path.parent.mkdir()\n",
    "\n",
    "with open(stats_pkl_path, 'wb') as f:\n",
    "    pickle.dump(case_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_boa_toa_quantities(ds, toa_level):\n",
    "    boa = ds.sel(half_level=slice(0, 1), level=slice(0, 1)).rename(level='half_level')\n",
    "    toa = ds.sel(half_level=slice(toa_level, toa_level + 1), level=slice(toa_level, toa_level + 1)).rename(level='half_level')\n",
    "    return xr.merge([\n",
    "        boa[['flux_dn_lw', 'flux_dn_sw', 'flux_dn_direct_sw']].rename({\n",
    "            'flux_dn_lw': 'flux_dn_lw_boa',\n",
    "            'flux_dn_sw': 'flux_dn_sw_boa',\n",
    "            'flux_dn_direct_sw': 'flux_dn_direct_sw_boa'\n",
    "        }),\n",
    "        toa[['flux_up_lw', 'flux_up_sw']].rename({\n",
    "            'flux_up_lw': 'flux_up_lw_toa',\n",
    "            'flux_up_sw': 'flux_up_sw_toa',\n",
    "        })\n",
    "    ])\n",
    "    \n",
    "def subset_levels(ds, toa_level):\n",
    "    return ds.sel(half_level=slice(0, toa_level + 1), level=slice(0, toa_level + 1)).rename(level='half_level')\n",
    "\n",
    "if level:\n",
    "    toa_level = level.stop\n",
    "else:\n",
    "    toa_level = d.y_sparta_test.level[-1].item()\n",
    "ordering = [\n",
    "    'flux_dn_lw', 'flux_up_lw',\n",
    "    'flux_dn_sw', 'flux_dn_direct_sw', 'flux_up_sw',\n",
    "    'heating_rate_lw', 'heating_rate_sw',\n",
    "    'flux_dn_lw_boa', 'flux_up_lw_toa',\n",
    "    'flux_dn_sw_boa', 'flux_dn_direct_sw_boa', 'flux_up_sw_toa'\n",
    "]\n",
    "stats_cols = ['mbe', 'mae', 'rmse', 'std']\n",
    "\n",
    "m_baseline = pd.concat([\n",
    "    utils.compute_error_metrics(\n",
    "        subset_levels(d.y_sparta_test if use_diff else d.y_sparta_test + d.y_triple_test, toa_level),\n",
    "        subset_levels(d.y_triple_test, toa_level))[stats_cols].drop(index='all'),\n",
    "    utils.compute_error_metrics(\n",
    "        derive_boa_toa_quantities(d.y_sparta_test if use_diff else d.y_sparta_test + d.y_triple_test, toa_level),\n",
    "        derive_boa_toa_quantities(d.y_triple_test, toa_level))[stats_cols].drop(index='all')\n",
    "]).reindex(ordering)\n",
    "\n",
    "m_error = pd.concat([\n",
    "    utils.compute_error_metrics(\n",
    "        subset_levels(d.y_true_test, toa_level),\n",
    "        subset_levels(d.y_pred_test, toa_level))[stats_cols].drop(index='all'),\n",
    "    utils.compute_error_metrics(\n",
    "        derive_boa_toa_quantities(d.y_true_test, toa_level),\n",
    "        derive_boa_toa_quantities(d.y_pred_test, toa_level))[stats_cols].drop(index='all')\n",
    "]).reindex(ordering)\n",
    "\n",
    "m_percent = m_error / m_baseline * 100\n",
    "\n",
    "def merge_multiindex_with_label(dfs, labels):\n",
    "    return pd.concat(dfs, axis=1,keys=labels).swaplevel(0,1,axis=1).sort_index(axis=1)\n",
    "\n",
    "m_table = merge_multiindex_with_label([m_baseline, m_error, m_percent], ['baseline', 'error', 'percent'])\n",
    "m_table = m_table.rename(columns={'mbe': '_mbe'}).sort_index(axis=1)\n",
    "\n",
    "def fmt(v):\n",
    "    s = '{0:.2g}'.format(v)\n",
    "    if 'e' in s:\n",
    "        v = float(s)\n",
    "        s = str(v).rstrip('0').rstrip('.')\n",
    "    return s\n",
    "\n",
    "with pd.option_context('display.float_format', fmt):\n",
    "    display(m_table)\n",
    "    header = [f'{m}_{n}' for m, n in m_table.columns]\n",
    "    print(m_table.to_csv(header=header, float_format='%.2g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_profiles(ds_out_true, ds_out_pred, n_rand_profiles):\n",
    "    # Ensure same results across multiple runs\n",
    "    np.random.seed(0)\n",
    "    idxs = np.random.choice(ds_out_true.column, n_rand_profiles)\n",
    "    quantities = [x for x in ds_out_true.keys()]\n",
    "    fig, ax = plt.subplots(len(idxs), len(quantities), figsize=(6*len(quantities), 3*len(idxs)))\n",
    "    for c_count, c_idx in enumerate(idxs):\n",
    "        for q_idx, q_name in enumerate(quantities):\n",
    "            ds_out_true[q_name].sel(column=c_idx).plot(ax=ax[c_count, q_idx], label='true', c='k', linestyle='--')\n",
    "            ds_out_pred[q_name].sel(column=c_idx).plot(ax=ax[c_count, q_idx], label='pred', c='r')\n",
    "            ax[c_count, q_idx].set_title(f'Column Index: {c_idx}')\n",
    "            ax[c_count, q_idx].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_profiles(d.y_true_test, d.y_pred_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(ds_3d_true, ds_3d_pred, ds_sparta, plot_x_absolute=False):\n",
    "    if use_diff:\n",
    "        plot_x_absolute = False\n",
    "    \n",
    "    d_names = {\n",
    "        'flux_dn_lw' : 'Downwelling longwave',\n",
    "        'flux_up_lw' : 'Upwelling longwave',\n",
    "        'flux_dn_sw' : 'Total downwelling shortwave',\n",
    "        'flux_dn_direct_sw' : 'Direct downwelling shortwave',\n",
    "        'flux_up_sw' : 'Upwelling shortwave'\n",
    "    }\n",
    "    \n",
    "    q_l_map = {'flux_dn_lw' : 0,\n",
    "             'flux_up_lw' : -1,\n",
    "             'flux_dn_sw' : 0,\n",
    "             'flux_dn_direct_sw' : 0,\n",
    "             'flux_up_sw' : -1}\n",
    "    fig, ax = plt.subplots(1, len(q_l_map), figsize=(6*len(q_l_map), 5))\n",
    "    for q_idx, q_name in enumerate(q_l_map.keys()):\n",
    "        if plot_x_absolute:\n",
    "            x = ds_3d_true[q_name].sel(half_level=q_l_map[q_name]) + ds_sparta[q_name].sel(half_level=q_l_map[q_name])\n",
    "        else:\n",
    "            x = ds_3d_true[q_name].sel(half_level=q_l_map[q_name])\n",
    "\n",
    "        y = ds_3d_pred[q_name].sel(half_level=q_l_map[q_name])\n",
    "        ax[q_idx].scatter(x=x,y=y, s=10, facecolors='none', edgecolors='r', alpha=0.5)\n",
    "        x_y_lim_min = xr.concat([x,y], dim='column').min()\n",
    "        x_y_lim_max = xr.concat([x,y], dim='column').max()\n",
    "    \n",
    "        # Line\n",
    "        if plot_x_absolute:\n",
    "            x_min = ds_sparta.flux_dn_sw.sel(half_level=q_l_map[q_name]).min()\n",
    "            x_max = ds_sparta.flux_dn_sw.sel(half_level=q_l_map[q_name]).max()\n",
    "            ax[q_idx].plot([x_min, x_max], [0,0], c='k', linestyle='--')\n",
    "\n",
    "            ax[q_idx].set_xlabel('Reference flux in W m⁻²')\n",
    "        else:\n",
    "            ax[q_idx].set_xlabel(f'Reference {d_names[q_name].lower()} in W m⁻²')\n",
    "        \n",
    "        ax[q_idx].set_ylabel(f'Predicted {d_names[q_name].lower()} in W m⁻²')\n",
    "        \n",
    "        l_name = 'BOA' if q_l_map[q_name] == 0 else 'TOA'\n",
    "        if use_diff:\n",
    "            ax[q_idx].set_title(f'3D radiative effect flux at {l_name}', fontweight='bold')\n",
    "        else:\n",
    "            ax[q_idx].set_title(f'Flux at {l_name}', fontweight='bold')\n",
    "        ax[q_idx].set_xlim(x_y_lim_min, x_y_lim_max)\n",
    "        ax[q_idx].set_ylim(x_y_lim_min, x_y_lim_max)\n",
    "        ax[q_idx].plot(ax[q_idx].get_xlim(), ax[q_idx].get_ylim(), ls=\"--\", c=\".3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(d.y_true_test, d.y_pred_test, d.y_sparta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atmos_abs(ds, flux_type):\n",
    "    net_pred = (ds[f'flux_dn_{flux_type}'].sel(half_level=-1) - ds[f'flux_up_{flux_type}'].sel(half_level=-1)) \\\n",
    "             - (ds[f'flux_dn_{flux_type}'].sel(half_level=0) - ds[f'flux_up_{flux_type}'].sel(half_level=0))\n",
    "    return net_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sw_true = compute_atmos_abs(d.y_true_test, 'sw')\n",
    "abs_sw_pred = compute_atmos_abs(d.y_pred_test, 'sw')\n",
    "abs_lw_true = compute_atmos_abs(d.y_true_test, 'lw')\n",
    "abs_lw_pred = compute_atmos_abs(d.y_pred_test, 'lw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_net_toa_boa(ds, flux_type):\n",
    "    tmp_toa = (ds[f'flux_dn_{flux_type}'].sel(half_level=-1) - ds[f'flux_up_{flux_type}'].sel(half_level=-1))\n",
    "    tmp_boa = (ds[f'flux_dn_{flux_type}'].sel(half_level=0) - ds[f'flux_up_{flux_type}'].sel(half_level=0))\n",
    "    _, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "    lim_min = min(tmp_toa.min(), tmp_boa.min())\n",
    "    lim_max = max(tmp_toa.max(), tmp_boa.max())\n",
    "    ax[0].scatter(tmp_boa, tmp_toa, alpha=0.1, color='k')\n",
    "    ax[0].set_xlabel('3D Effect on net BOA SW flux in W/m2')\n",
    "    ax[0].set_ylabel('3D effect on net TOA SW flux  in W/m2')\n",
    "    ax[0].set_ylim([lim_min,lim_max])\n",
    "    ax[0].set_xlim([lim_min,lim_max])\n",
    "    ax[0].axline((0, 0), (1, 1), linewidth=1, color='r')\n",
    "    \n",
    "    ax[1].scatter(tmp_boa, abs_sw_true, alpha=0.1, color='k')\n",
    "    ax[1].set_xlabel('3D Effect on net BOA SW flux in W/m2')\n",
    "    ax[1].set_ylabel('3D effect on net SW atmospheric absorption in W/m2')\n",
    "    ax[1].set_ylim([lim_min,lim_max])\n",
    "    ax[1].set_xlim([lim_min,lim_max])\n",
    "    ax[1].axline((0, 0), (1, 1), linewidth=1, color='r')\n",
    "\n",
    "plot_net_toa_boa(d.y_true_test, 'sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "lim_min = min(abs_sw_true.min(), abs_sw_pred.min())\n",
    "lim_max = max(abs_sw_true.max(), abs_sw_pred.max())\n",
    "ax[0].scatter(abs_sw_true, abs_sw_pred, alpha=0.1, color='k')\n",
    "ax[0].set_xlabel('3D signal in W/m2')\n",
    "ax[0].set_ylabel('3D prediction in W/m2')\n",
    "ax[0].set_title('Atmospheric Absorption (Shortwave)')\n",
    "ax[0].set_ylim([lim_min,lim_max])\n",
    "ax[0].set_xlim([lim_min,lim_max])\n",
    "ax[0].axline((0, 0), (1, 1), linewidth=1, color='r')\n",
    "\n",
    "lim_min = min(abs_lw_true.min(), abs_lw_pred.min())\n",
    "lim_max = max(abs_lw_true.max(), abs_lw_pred.max())\n",
    "ax[1].scatter(abs_lw_true, abs_lw_pred, alpha=0.1, color='k')\n",
    "ax[1].set_xlabel('3D signal in W/m2')\n",
    "ax[1].set_ylabel('3D prediction in W/m2')\n",
    "ax[1].set_title('Atmospheric Absorption (Longwave)')\n",
    "ax[1].set_ylim([lim_min,lim_max])\n",
    "ax[1].set_xlim([lim_min,lim_max])\n",
    "ax[1].axline((0, 0), (1, 1), linewidth=1, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "pred_ref_dn_sw_boa = d.y_pred_test['flux_dn_sw'].sel(half_level=0) * d.x_test['sw_albedo']\n",
    "pred_com_dn_sw_boa = d.y_pred_test['flux_up_sw'].sel(half_level=0)\n",
    "\n",
    "lim_min = min(pred_ref_dn_sw_boa.min(), pred_com_dn_sw_boa.min())\n",
    "lim_max = max(pred_ref_dn_sw_boa.max(), pred_com_dn_sw_boa.max())\n",
    "ax[0].scatter(pred_ref_dn_sw_boa, pred_com_dn_sw_boa, alpha=0.1, color='k')\n",
    "ax[0].set_xlabel('3D S DN at BOA x Surface Albedo in W/m2')\n",
    "ax[0].set_ylabel('3D S DN at BOA')\n",
    "ax[0].set_title('3D predictions')\n",
    "ax[0].set_ylim([lim_min,lim_max])\n",
    "ax[0].set_xlim([lim_min,lim_max])\n",
    "ax[0].axline((0, 0), (1, 1), linewidth=1, color='r')\n",
    "\n",
    "true_ref_dn_sw_boa = d.y_true_test['flux_dn_sw'].sel(half_level=0) * d.x_test['sw_albedo']\n",
    "true_com_dn_sw_boa = d.y_true_test['flux_up_sw'].sel(half_level=0)\n",
    "\n",
    "lim_min = min(true_ref_dn_sw_boa.min(), true_com_dn_sw_boa.min())\n",
    "lim_max = max(true_ref_dn_sw_boa.max(), true_com_dn_sw_boa.max())\n",
    "ax[1].scatter(true_ref_dn_sw_boa, true_com_dn_sw_boa, alpha=0.1, color='k')\n",
    "ax[1].set_xlabel('3D S DN at BOA x Surface Albedo in W/m2')\n",
    "ax[1].set_ylabel('3D S UP at BOA')\n",
    "ax[1].set_title('3D signal (SPARTACUS - Tripleclouds)')\n",
    "ax[1].set_ylim([lim_min,lim_max])\n",
    "ax[1].set_xlim([lim_min,lim_max])\n",
    "ax[1].axline((0, 0), (1, 1), linewidth=1, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import multi_plot\n",
    "y_sparta = d.y_sparta_test.sel(half_level=slice(0, -1)).rename(half_level='level')\n",
    "y_triple = d.y_triple_test.sel(half_level=slice(0, -1)).rename(half_level='level')\n",
    "if use_diff:\n",
    "    y_nn = d.y_triple_test.sel(half_level=slice(0, -1)).rename(half_level='level') \\\n",
    "         + d.y_pred_test.sel(half_level=slice(0, -1)).rename(half_level='level')\n",
    "else:\n",
    "    y_nn = d.y_pred_test.sel(half_level=slice(0, -1)).rename(half_level='level')\n",
    "x_true = d.x_test\n",
    "x_true = x_true.assign(pressure_fl=x_true['pressure_fl'] / 100)\n",
    "\n",
    "utils.multi_plot(y_sparta, y_triple, y_nn, x_true, \n",
    "                     y_axis='pressure_fl', y_axis_label='Pressure in hPa',\n",
    "                     variant='fluxes', is_diff=use_diff)\n",
    "utils.plt_show_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.multi_plot(y_sparta, y_triple, y_nn, x_true, \n",
    "                     y_axis='pressure_fl', y_axis_label='Pressure in hPa',\n",
    "                     variant='hr', is_diff=use_diff)\n",
    "utils.plt_show_svg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Era5slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_era5 = path_data / 'era5slice'\n",
    "path_era5_in =  path_era5 / 'era5slice.nc'\n",
    "\n",
    "path_ecrad = path_proj / 'ecrad'\n",
    "path_ecrad_bin = path_ecrad / 'bin' / 'ecrad'\n",
    "\n",
    "path_triple_era5_nml = path_era5 / 'config_era5slice_tripleclouds.nam'\n",
    "path_sparta_era5_nml = path_era5 / 'config_era5slice_spartacus.nam'\n",
    "\n",
    "path_era5_out_dir = Path(tempfile.gettempdir()) if pbs_id else path_era5\n",
    "path_triple_era5_out = path_era5_out_dir / 'ecrad_nwp_era5slice_tripleclouds_out.nc'\n",
    "path_sparta_era5_out = path_era5_out_dir / 'ecrad_nwp_era5slice_spartacus_out.nc'\n",
    "\n",
    "nn_out_path = path_era5_out_dir /  'era5slice_nn_out.nc'\n",
    "\n",
    "windows_paths_era5 = [path_ecrad, path_ecrad_bin, path_triple_era5_nml, path_sparta_era5_nml, \n",
    "                      path_era5_in, path_triple_era5_out, path_sparta_era5_out, path_logs]\n",
    "posix_paths_era5 = []    \n",
    "for path in windows_paths_era5:\n",
    "    if is_windows:\n",
    "        x = ! wsl wslpath \"{path}\"\n",
    "        x = x[0]\n",
    "    else:\n",
    "        x = path.as_posix()\n",
    "    posix_paths_era5.append(x)\n",
    "\n",
    "plots_path = path_era5_out_dir / 'plots'\n",
    "ecrad_plots_path = plots_path / 'ecrad'\n",
    "nn_plots_path = plots_path / 'nn'\n",
    "shutil.rmtree(ecrad_plots_path, ignore_errors=True)\n",
    "ecrad_plots_path.mkdir(parents=True)\n",
    "shutil.rmtree(nn_plots_path, ignore_errors=True)\n",
    "nn_plots_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Spartacus\n",
    "    # ! cd {ecrad_dir} && wsl OMP_NUM_THREADS= {ecrad} {sparta_nml_path} {derived_input_path} {sparta_output_path} > sparta_synth.log\n",
    "    ! cd {path_ecrad} && {wsl} OMP_NUM_THREADS= {posix_paths_era5[1]} {posix_paths_era5[3]} {posix_paths_era5[4]} {posix_paths_era5[6]} > {windows_paths_era5[7]}/sparta_synth__era_era5.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enhance_data:\n",
    "    # Tripleclouds\n",
    "    ! cd {path_ecrad} && {wsl} OMP_NUM_THREADS= {posix_paths_era5[1]} {posix_paths_era5[2]} {posix_paths_era5[4]} {posix_paths_era5[5]} > {windows_paths_era5[7]}/triple_synth_era5.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_out_path = path_triple_era5_out\n",
    "sparta_out_path = path_sparta_era5_out\n",
    "in_path = path_era5_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data_all = utils.load_scheme_inputs(path_era5_in, only_relevant=False)\n",
    "\n",
    "ds_inputs = utils.add_derived_inputs(in_data_all)\n",
    "ds_inputs = ds_inputs[input_quantities]\n",
    "if level:\n",
    "    ds_inputs = ds_inputs.sel(level=level)\n",
    "\n",
    "if normalize_inputs:\n",
    "    ds_inputs = utils.normalize_inputs(ds_inputs, norm_stats_in)\n",
    "    \n",
    "utils.plot_inputs(ds_inputs, normalize_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_lw_sw_split:\n",
    "    # Longwave\n",
    "    in_data_lw = ds_inputs.drop(['sw_albedo', 'cos_solar_zenith_angle'])\n",
    "    if model_type == 'RNN':\n",
    "        in_data_lw = xr_1d_to_2d_vars(in_data_lw, ['skin_temperature'])\n",
    "    triple_out_lw = utils.load_scheme_outputs(triple_out_path, only_relevant=True)[lw_fluxes]\n",
    "    if use_heating_rates:\n",
    "        triple_out_lw = utils.add_heating_rates(triple_out_lw, in_data_all)\n",
    "\n",
    "    x_flat_lw = d_lw.model.to_model_input(in_data_lw)\n",
    "    y_pred_lw = d_lw.model.predict(x_flat_lw)\n",
    "    if normalize_outputs:\n",
    "        y_pred_lw = utils.unnormalize_outputs(y_pred_lw, norm_stats_out_lw)\n",
    "    if use_diff:\n",
    "        y_pred_lw = recover_levels(y_pred_lw, triple_out_lw)\n",
    "        y_pred_lw = triple_out_lw + y_pred_lw\n",
    "    \n",
    "    # Shortwave\n",
    "    in_data_sw = ds_inputs.drop(['skin_temperature', 'temperature_fl'])\n",
    "    if model_type == 'RNN':\n",
    "        in_data_sw = xr_1d_to_2d_vars(in_data_sw, ['cos_solar_zenith_angle', 'sw_albedo'])\n",
    "    triple_out_sw = utils.load_scheme_outputs(triple_out_path, only_relevant=True)[sw_fluxes]\n",
    "    if use_heating_rates:\n",
    "        triple_out_sw = utils.add_heating_rates(triple_out_sw, in_data_all)\n",
    "\n",
    "    x_flat_sw = d_sw.model.to_model_input(in_data_sw)\n",
    "    y_pred_sw = d_sw.model.predict(x_flat_sw)\n",
    "    if normalize_outputs:\n",
    "        y_pred_sw = utils.unnormalize_outputs(y_pred_sw, norm_stats_out_sw)\n",
    "    if use_diff:\n",
    "        y_pred_sw = recover_levels(y_pred_sw, triple_out_sw)\n",
    "        y_pred_sw = triple_out_sw + y_pred_sw\n",
    "    \n",
    "    # Merge/load all\n",
    "    y_pred = xr.merge([y_pred_lw, y_pred_sw])\n",
    "    in_data = utils.load_scheme_inputs(in_path, only_relevant=True)\n",
    "    triple_out = utils.load_scheme_outputs(triple_out_path, only_relevant=True)\n",
    "    sparta_out = utils.load_scheme_outputs(sparta_out_path, only_relevant=True)\n",
    "\n",
    "else:\n",
    "    in_data = ds_inputs\n",
    "    triple_out = utils.load_scheme_outputs(triple_out_path, only_relevant=True)\n",
    "    sparta_out = utils.load_scheme_outputs(sparta_out_path, only_relevant=True)\n",
    "\n",
    "    x_flat = d.model.to_model_input(in_data)\n",
    "    y_pred = d.model.predict(x_flat)\n",
    "    if normalize_outputs:\n",
    "        y_pred = utils.unnormalize_outputs(y_pred, norm_stats_out)\n",
    "    if use_diff:\n",
    "        y_pred = recover_levels(y_pred, triple_out)\n",
    "        y_pred = triple_out + y_pred\n",
    "    \n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over clear-sky fluxes from Tripleclouds.\n",
    "# Since 3D effects arise only when clouds are present, the clear-sky fluxes\n",
    "# are identical between Tripleclouds and SPARTACUS.\n",
    "\n",
    "triple_out_temp = utils.load_scheme_outputs(triple_out_path, only_relevant=False)\n",
    "if use_heating_rates:\n",
    "    triple_out_temp = utils.add_heating_rates(triple_out_temp, in_data_all)\n",
    "\n",
    "clear_sky_fluxes = ['flux_up_lw_clear', 'flux_dn_lw_clear', 'flux_up_sw_clear', 'flux_dn_sw_clear', 'cloud_cover_sw']\n",
    "y_pred_temp = y_pred.merge(triple_out_temp[clear_sky_fluxes])\n",
    "\n",
    "# Copy over profiles from Tripleclouds for all non-cloudy input profiles.\n",
    "if cloudy_profiles_only:\n",
    "    non_cloudy_idx = sel_cloudy_profiles(in_data_all, cloudy=False).column\n",
    "    for var_name in y_pred_temp:\n",
    "        y_pred_temp[var_name].loc[dict(column=non_cloudy_idx)] = triple_out_temp[var_name].sel(column=non_cloudy_idx)\n",
    "\n",
    "# store as NetCDF for \n",
    "y_pred_reversed = utils.reverse_levels(y_pred_temp)\n",
    "y_pred_reversed.to_netcdf(nn_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "# add --include-t to plot T as second y axis\n",
    "stdout = ! python {path_ecrad}/practical/plot_input.py {in_path} --mode scalars --dstdir {ecrad_plots_path}\n",
    "fig_path = stdout[0].split()[-1]\n",
    "Image(filename=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D effect: True and Predicted\n",
    "stdout = ! python {path_ecrad}/practical/compare_output.py {in_path} {triple_out_path} {sparta_out_path} --output2 {nn_out_path} --dstdir {ecrad_plots_path}\n",
    "fig_path = stdout[0].split()[-1]\n",
    "Image(filename=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN error\n",
    "stdout = ! python {path_ecrad}/practical/compare_output.py {in_path} {sparta_out_path} {nn_out_path} --dstdir {nn_plots_path}\n",
    "fig_path = stdout[0].split()[-1]\n",
    "Image(filename=fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOA\n",
    "# T\n",
    "stdout = ! python {path_ecrad}/practical/compare_output_scalar.py {in_path} {triple_out_path} {sparta_out_path} {nn_out_path} --mode paper --dstdir {nn_plots_path}\n",
    "fig_path = stdout[0].split()[-1]\n",
    "utils.plt_show_svg(fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = utils.add_heating_rates(y_pred, in_data_all)\n",
    "triple_out = utils.add_heating_rates(triple_out, in_data_all)\n",
    "sparta_out = utils.add_heating_rates(sparta_out, in_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_profiles(sparta_out - triple_out, y_pred - triple_out, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(sparta_out - triple_out, y_pred - triple_out, sparta_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if level:\n",
    "    m_era5slice = utils.compute_error_metrics(y_pred.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1))[stat_quantities_sw + stat_quantities_lw].drop(_heating_rate_sw + _heating_rate_lw),\n",
    "                                          sparta_out.sel(half_level=slice(level.start, level.stop + 1), level=slice(level.start, level.stop + 1))[stat_quantities_sw + stat_quantities_lw].drop(_heating_rate_sw + _heating_rate_lw)).add_prefix('era5slice_')\n",
    "else:\n",
    "    m_era5slice = utils.compute_error_metrics(y_pred[stat_quantities_sw + stat_quantities_lw].drop(_heating_rate_sw + _heating_rate_lw),\n",
    "                                          sparta_out[stat_quantities_sw + stat_quantities_lw].drop(_heating_rate_sw + _heating_rate_lw)).add_prefix('era5slice_')\n",
    "m_era5slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_pbs_job:\n",
    "    f_job_failure.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
